{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA/RNA Virus Toolklit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.sciencealert.com/images/2020-03/processed/virus_topic_1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import os, sys, traceback\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.Alphabet import generic_dna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import tree\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structures needed in genome analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare valid nucleotides, proteins and their complements, as well as uncertain letters appearing in sequences from NCBI\n",
    "nucleotides = [\"A\", \"T\", \"C\", \"G\"]\n",
    "proteins = ['A','D','E','G','F','L','S','Y','C','W','L','P','H','Q','R','I','M','T','N','K','S','R','V']\n",
    "complements = {\"A\": \"T\", \"T\": \"A\", \"G\": \"C\", \"C\": \"G\"}\n",
    "uncertainties = ['W', 'U', 'S', 'M', 'K', 'R', 'Y', 'B', 'D', 'H', 'V', 'N', 'Z']\n",
    "amino_acids = {\n",
    "    'alanine': 'A', 'arginine': 'R', 'Asparagine': 'N', 'Aspartic Acid': 'D',\n",
    "    'Cysteine': 'C', 'Glutamic Acid': 'E', 'Glutamine': 'Q',\n",
    "    'Glycine': 'G', 'Histidine': 'H', 'Isoleucine': 'I', 'Leucine': 'L',\n",
    "    'Lysine': 'K', 'Methionine': 'M', 'Phenylalanine': 'F', 'Proline': 'P',\n",
    "    'Serine': 'S', 'Threonine': 'T', 'Tryptophan': 'W', 'Tyrosine': 'Y',\n",
    "    'Valine': 'V'\n",
    "}\n",
    "\n",
    "# using mRNA in lookup, not tRNA anti-codon\n",
    "start_codons = [\"ATG\"]\n",
    "stop_codons = [\"TAA\", \"TAG\", \"TGA\"]\n",
    "dna_codons = {\n",
    "    # 'M' - START, '_' - STOP\n",
    "    \"GCT\": \"A\", \"GCC\": \"A\", \"GCA\": \"A\", \"GCG\": \"A\",\n",
    "    \"TGT\": \"C\", \"TGC\": \"C\",\n",
    "    \"GAT\": \"D\", \"GAC\": \"D\",\n",
    "    \"GAA\": \"E\", \"GAG\": \"E\",\n",
    "    \"TTT\": \"F\", \"TTC\": \"F\",\n",
    "    \"GGT\": \"G\", \"GGC\": \"G\", \"GGA\": \"G\", \"GGG\": \"G\",\n",
    "    \"CAT\": \"H\", \"CAC\": \"H\",\n",
    "    \"ATA\": \"I\", \"ATT\": \"I\", \"ATC\": \"I\",\n",
    "    \"AAA\": \"K\", \"AAG\": \"K\",\n",
    "    \"TTA\": \"L\", \"TTG\": \"L\", \"CTT\": \"L\", \"CTC\": \"L\", \"CTA\": \"L\", \"CTG\": \"L\",\n",
    "    \"ATG\": \"M\",\n",
    "    \"AAT\": \"N\", \"AAC\": \"N\",\n",
    "    \"CCT\": \"P\", \"CCC\": \"P\", \"CCA\": \"P\", \"CCG\": \"P\",\n",
    "    \"CAA\": \"Q\", \"CAG\": \"Q\",\n",
    "    \"CGT\": \"R\", \"CGC\": \"R\", \"CGA\": \"R\", \"CGG\": \"R\", \"AGA\": \"R\", \"AGG\": \"R\",\n",
    "    \"TCT\": \"S\", \"TCC\": \"S\", \"TCA\": \"S\", \"TCG\": \"S\", \"AGT\": \"S\", \"AGC\": \"S\",\n",
    "    \"ACT\": \"T\", \"ACC\": \"T\", \"ACA\": \"T\", \"ACG\": \"T\",\n",
    "    \"GTT\": \"V\", \"GTC\": \"V\", \"GTA\": \"V\", \"GTG\": \"V\",\n",
    "    \"TGG\": \"W\",\n",
    "    \"TAT\": \"Y\", \"TAC\": \"Y\",\n",
    "    \"TAA\": \"_\", \"TAG\": \"_\", \"TGA\": \"_\"\n",
    "}\n",
    "\n",
    "rna_codons = {\n",
    "    # 'M' - START, '_' - STOP\n",
    "    \"GCU\": \"A\", \"GCC\": \"A\", \"GCA\": \"A\", \"GCG\": \"A\",\n",
    "    \"UGU\": \"C\", \"UGC\": \"C\",\n",
    "    \"GAU\": \"D\", \"GAC\": \"D\",\n",
    "    \"GAA\": \"E\", \"GAG\": \"E\",\n",
    "    \"UUU\": \"F\", \"UUC\": \"F\",\n",
    "    \"GGU\": \"G\", \"GGC\": \"G\", \"GGA\": \"G\", \"GGG\": \"G\",\n",
    "    \"CAU\": \"H\", \"CAC\": \"H\",\n",
    "    \"AUA\": \"I\", \"AUU\": \"I\", \"AUC\": \"I\",\n",
    "    \"AAA\": \"K\", \"AAG\": \"K\",\n",
    "    \"UUA\": \"L\", \"UUG\": \"L\", \"CUU\": \"L\", \"CUC\": \"L\", \"CUA\": \"L\", \"CUG\": \"L\",\n",
    "    \"AUG\": \"M\",\n",
    "    \"AAU\": \"N\", \"AAC\": \"N\",\n",
    "    \"CCU\": \"P\", \"CCC\": \"P\", \"CCA\": \"P\", \"CCG\": \"P\",\n",
    "    \"CAA\": \"Q\", \"CAG\": \"Q\",\n",
    "    \"CGU\": \"R\", \"CGC\": \"R\", \"CGA\": \"R\", \"CGG\": \"R\", \"AGA\": \"R\", \"AGG\": \"R\",\n",
    "    \"UCU\": \"S\", \"UCC\": \"S\", \"UCA\": \"S\", \"UCG\": \"S\", \"AGU\": \"S\", \"AGC\": \"S\",\n",
    "    \"ACU\": \"T\", \"ACC\": \"T\", \"ACA\": \"T\", \"ACG\": \"T\",\n",
    "    \"GUU\": \"V\", \"GUC\": \"V\", \"GUA\": \"V\", \"GUG\": \"V\",\n",
    "    \"UGG\": \"W\",\n",
    "    \"UAU\": \"Y\", \"UAC\": \"Y\",\n",
    "    \"UAA\": \"_\", \"UAG\": \"_\", \"UGA\": \"_\"\n",
    "}\n",
    "\n",
    "# two test sequences for the below methods, one is COVID, and the other is deformed wing disease (avian), virulent and non-virulent respectively\n",
    "BT006808_1 = \"ATGGCCCTGTGGATGCGCCTCCTGCCCCTGCTGGCGCTGCTGGCCCTCTGGGGACCTGACCCAGCCGCAGCCTTTGTGAACCAACACCTGTGCGGCTCACACCTGGTGGAAGCTCTCTACCTAGTGTGCGGGGAACGAGGCTTCTTCTACACACCCAAGACCCGCCGGGAGGCAGAGGACCTGCAGGTGGGGCAGGTGGAGCTGGGCGGGGGCCCTGGTGCAGGCAGCCTGCAGCCCTTGGCCCTGGAGGGGTCCCTGCAGAAGCGTGGCATTGTGGAACAATGCTGTACCAGCATCTGCTCCCTCTACCAGCTGGAGAACTACTGCAACTAG\"\n",
    "MT324680_1 = \"CAAKACTCACTTTCTTCCACAGCAAGTGCACTTGGAAAACTTCAAGATGTGGTCAACCAAAATGCACAAGCTTTAAACACGCTTGTTAAACAACTTAGCTCCAATTTTGGTGCAATTTCAAGTGTTTTAAATGATATCCTTTCACGTCTTGACAAAGTTGAGGCTGAAGTGCAAATTGATAGGTTGATCACAGGCAGACTTCAAAGTTTGCAGACATATGTGACTCAACAATTAATTAGAGCTGCAGAAATCAGAGCTTCTGCTAATCTTGCTGCTACTAAAATGTCAGAGTGTGTACTTGGACAATCAAAAAGAGTTGATTTTTGTGGAAAGGGCTATCATCTTATGTCCTTCCCTCAGTCAGCACCTCATGGTGTAGTCTTCTTGCATGTGACTTATGTCCCTGCACAAGAAAAGAACTTCACAACTGCTCCTGCCATTTGTCATGATGGAAAAGCACACTTTCCTCGTGAAGGTGTCTTTGTTTCAAA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined bioinformatic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateNucleotides(seq):\n",
    "    \"\"\"\n",
    "    input: string sequence from fasta file or string input\n",
    "    operation: translates nucleotide sequence into protein sequence, and removes unknown characters\n",
    "    output: returns a clean translated protein sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    seq = Seq(seq, generic_dna)\n",
    "    translated_seq = str(seq.translate()).replace('*', '')\n",
    "    clean_seq = ''.join([p for p in translated_seq if p in proteins])\n",
    "    return clean_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVirusDf(fasta_data):\n",
    "    \"\"\"\n",
    "    input: fasta data file uploaded by user\n",
    "    operation: after sequence translation, uses protein analysis \\\n",
    "       and other custom functions to calcuate metrics used in virology research \\\n",
    "    output: returns a dataframe with a feature/column for each engineered feature\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ids = [] # accession id, searchable on NCBI\n",
    "        descs = [] # text description of strain\n",
    "        aa_percs = [] # ratio of amino acids in strain\n",
    "        m_weights = [] # molecular weight\n",
    "        a = [] # aromaticity\n",
    "        ii = [] # instability index, > 40 means unstable / short half life .instability_index()\n",
    "        n_seqs = [] # nuleotide sequence\n",
    "        p_seqs = [] # translated protein sequence\n",
    "\n",
    "        for record in tqdm(fasta_data):\n",
    "            print(f'Processing {record.id} - {record.description}')\n",
    "#             str_seq = cleanPlaceholderNucleotides(str(record.seq))\n",
    "            str_seq = str(record.seq)\n",
    "    \n",
    "            # translated nucleotide string sequence\n",
    "            p_seq = translateNucleotides(str_seq)\n",
    "            ids.append(record.id)\n",
    "            descs.append(record.description)\n",
    "            \n",
    "            # GC content\n",
    "            gc_content = calcGcContent(str_seq)\n",
    "            \n",
    "            # protein analysis methods\n",
    "            analysis = ProteinAnalysis(p_seq)\n",
    "            aa_percs.append(analysis.get_amino_acids_percent())\n",
    "            m_weights.append(analysis.molecular_weight())\n",
    "            ii.append(analysis.instability_index())\n",
    "            a.append(analysis.aromaticity())\n",
    "            \n",
    "            # nucleotide and protein string sequences\n",
    "            n_seqs.append(str_seq)\n",
    "            p_seqs.append(str(record.seq.translate()).replace('*', ' '))\n",
    "\n",
    "        virus_df = pd.DataFrame({\n",
    "            \"ids\": ids,\n",
    "            \"description\": descs,\n",
    "            \"gc_content\": gc_content,\n",
    "            \"amino_acid_percents\": aa_percs,\n",
    "            \"molecular_weights\": m_weights,\n",
    "            \"aromaticity\": a,\n",
    "            \"instability_index\": ii,\n",
    "            \"nucleotide_sequence\": n_seqs,\n",
    "            \"protein_sequence\": p_seqs\n",
    "        })\n",
    "\n",
    "        return virus_df\n",
    "                  \n",
    "    except Exception as e:\n",
    "        print('-'*80)\n",
    "        print(f\"Exception in building virus datatframe: {e}\")\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function for taking nucleotide string sequence and translating it to protein sequence\n",
    "def proteinTranslation(x):\n",
    "    return str(Seq(x).translate()).replace('*', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually defined function to return all of the protein analysis components\n",
    "def proteinAnalysis(s):\n",
    "    aa_percent, molecular_weight, instability_index, aromaticity = [], [], [], []\n",
    "    for x in tqdm(s):\n",
    "        protein_analysis = ProteinAnalysis(x.replace(' ', ''))\n",
    "        aa_percent.append(protein_analysis.get_amino_acids_percent())\n",
    "        molecular_weight.append(protein_analysis.molecular_weight())\n",
    "        instability_index.append(protein_analysis.instability_index())\n",
    "        aromaticity.append(protein_analysis.aromaticity())\n",
    "    return aa_percent, molecular_weight, instability_index, aromaticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes sequences with string length outside of 1.5IQR range\n",
    "def filterOutliers(df):\n",
    "    q1 = df.seq_len.quantile(.25)\n",
    "    q3 = df.seq_len.quantile(.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - iqr * 1.5\n",
    "    upper_bound = q3 + iqr * 1.5\n",
    "    \n",
    "    print(f'Lower IQR bound: {lower_bound}, Upper IQR bound: {upper_bound}')\n",
    "    \n",
    "    return df[(df.seq_len<=upper_bound) & (df.seq_len>=lower_bound)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to remove a string buffer for invalid sequences\n",
    "def removeStringBuffer(seq, i):\n",
    "    out_seq = seq[:i-i%3] + seq[i+3-i%3:]\n",
    "    return out_seq.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to clean placeholder characters from nucleotide sequence, later not used because only passing valid sequences\n",
    "def cleanPlaceholderNucleotides(seq):\n",
    "    k_pos = []\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] not in nucleotides:\n",
    "            print(f'Found placeholder at {i}th nucleotide')\n",
    "            k_pos.append(i)\n",
    "    \n",
    "    k_seq = seq\n",
    "    if len(k_pos) > 0:\n",
    "        for pos in k_pos:\n",
    "            k_seq = removeStringBuffer(k_seq, pos)\n",
    "\n",
    "    return k_seq[:(len(k_seq)-len(k_seq)%3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of amino acid triples\n",
    "def buildAminoTriples(s):\n",
    "    seq_list = []\n",
    "    for i in range(0, len(s), 3):\n",
    "        seq_list.append(s[i:i+3])\n",
    "    return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary with count of all nucleotides in string sequence\n",
    "def countNucleotides(seq):\n",
    "#     a_num = seq.count('A')\n",
    "#     t_num = seq.count('T')\n",
    "#     c_num = seq.count('C')\n",
    "#     g_num = seq.count('G')\n",
    "#     gc_content = (c_num + g_num) / (c_num + g_num + a_num + c_num)\n",
    "#     nuc_dict = {\"A\": a_num,\n",
    "#                \"T\": t_num,\n",
    "#                \"C\": c_num,\n",
    "#                \"G\": g_num,\n",
    "#                \"GC\": gc_content}\n",
    "    nuc_dict = dict(Counter(seq))\n",
    "    return nuc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validates if a sequence contains only valid DNA nucleotides (A, C, T, G)\n",
    "def validateString(seq):\n",
    "    tmp_seq = seq.upper()\n",
    "    for nuc in tmp_seq:\n",
    "        if nuc not in nucleotides:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a random sequence of DNA nucleotides\n",
    "def generateRandomString(num_nucs):\n",
    "    rand_seq = ''.join([random.choice(nucleotides) for nuc in range(num_nucs)])\n",
    "    return rand_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribes a nucleotide sequence from DNA to RNA (aka swapping T for U)\n",
    "def transcription(seq):\n",
    "    return seq.replace(\"T\", \"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the reverse complement of the nucleotide sequence\n",
    "def reverseComplement(seq):\n",
    "    return ''.join([complements[nuc] for nuc in seq])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the GC content of the nucleotide sequence\n",
    "def calcGcContent(seq):\n",
    "    return round((seq.count(\"C\") + seq.count(\"G\")) / len(seq) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the GC content of a subsequence, default range 20\n",
    "def subseqGcContent(seq, k=20):\n",
    "    res = []\n",
    "    for i in range(0, len(seq) - k + 1, k):\n",
    "        subseq = seq[i:i + k]\n",
    "        res.append(calcGcContent(subseq))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translates DNA or RNA nucleotide sequence to proteins by looking up codon dictionary values above\n",
    "def translateSequence(seq, seq_type='dna', init_pos=0):\n",
    "    if seq_type == 'dna':\n",
    "        return [dna_codons[seq[pos:pos+3]] for pos in range(init_pos, len(seq) -2, 3)]\n",
    "    elif seq_type == 'rna':\n",
    "        return [rna_condons[seq[pos:pos+3]] for pos in range(init_pos, len(seq) -2, 3)]\n",
    "    else:\n",
    "        return ['invalid genome type provided']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifies the frequency of codons in an amino acid string\n",
    "def codonUsage(seq, aminoacid):\n",
    "    codon_list = []\n",
    "    for i in range(0, len(seq) - 2, 3):\n",
    "        if dna_codons[seq[i:i + 3]] == aminoacid:\n",
    "            codon_list.append(seq[i:i + 3])\n",
    "            \n",
    "    codon_frequencies = dict(Counter(codon_list))\n",
    "    total_weight = sum(codon_frequencies.values())\n",
    "    for seq in codon_frequencies:\n",
    "        codon_frequencies[seq] = round(codon_frequencies[seq] / total_weight, 2)\n",
    "    return codon_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates reading frames from protein sequence\n",
    "def generateReadingFrames(seq):\n",
    "    frames = []\n",
    "    frames.append(translateSequence(seq, 'dna', 0))\n",
    "    frames.append(translateSequence(seq, 'dna', 1))\n",
    "    frames.append(translateSequence(seq, 'dna', 2))\n",
    "    frames.append(translateSequence(reverseComplement(seq), 'dna', 0))\n",
    "    frames.append(translateSequence(reverseComplement(seq), 'dna', 1))\n",
    "    frames.append(translateSequence(reverseComplement(seq), 'dna', 2))\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts protein sequence from the amino acid string, uses codon triplets\n",
    "def proteinsFromReadingFrame(amino_acids):\n",
    "    current_protein = []\n",
    "    proteins = []\n",
    "    \n",
    "    for amino_acid in amino_acids:\n",
    "        if amino_acid == \"_\":\n",
    "            if current_protein:\n",
    "                for protein in current_protein:\n",
    "                    proteins.append(protein)\n",
    "                current_protein = []\n",
    "        else:\n",
    "            if amino_acid == \"M\":\n",
    "                current_protein.append(\"\")\n",
    "            for i in range(len(current_protein)):\n",
    "                current_protein[i] += amino_acid\n",
    "    return proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates proteins from reading frames into a complete protein sequence\n",
    "def aggregateProteinsFromReadingFrames(seq, start_pos=0, end_pos=0, ordered=False):\n",
    "    if end_pos > start_pos:\n",
    "        reading_frames = generateReadingFrames(seq[start_pos: end_pos])\n",
    "    else:\n",
    "        reading_frames = generateReadingFrames(seq)\n",
    "    \n",
    "    result = []\n",
    "    for frame in reading_frames:\n",
    "        proteins = proteinsFromReadingFrame(frame)\n",
    "        for protein in proteins:\n",
    "            result.append(protein)\n",
    "            \n",
    "    if ordered:\n",
    "        return sorted(result, key=len, reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV files of valid sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the sequence_splitter.ipynb, two output CSV files were created.  Starting with the cell below, these two files of valid and complete virus DNA sequences will be imported into this environment to run analysis.  The source data for this analysis was obtained by downloading nucleotide sequences in FASTA file format from the National Institute of Health's NCBI Virus database.  For example, here is a link that shows where data for the complete COVID genome sequences were obtained:\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Nucleotide&VirusLineage_ss=Severe%20acute%20respiratory%20syndrome%20coronavirus%202,%20taxid:2697049&SourceDB_s=GenBank&Completeness_s=complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "non_vir_df = pd.read_csv('valid_non_virulent_sequences.csv') # archive of valid (only ATCG) non-virulent sequences\n",
    "vir_df = pd.read_csv('valid_virulent_sequences.csv') # archive of valid (only ATCG) non-virulent sequences\n",
    "\n",
    "print(f'Took {time.time() - t1} seconds to upload data files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give virulent df a categorical class, change to category type and add column for sequence length\n",
    "vir_df['class'] = 'virulent'\n",
    "vir_df['class'] = vir_df['class'].astype('category')\n",
    "vir_df['seq_len'] = vir_df['seq_str'].apply(lambda x: len(x))\n",
    "\n",
    "# give non-virulent df a categorical class, change to category type and add column for sequence length\n",
    "non_vir_df['class'] = 'non-virulent'\n",
    "non_vir_df['class'] = non_vir_df['class'].astype('category')\n",
    "non_vir_df['seq_len'] = non_vir_df['seq_str'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the imported files, we can see there is a roughly 3:1 split between virulent and non-virulent genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_vir_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View distribution of length of DNA/RNA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_vir_df.seq_len.plot(kind='box', rot=90, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_vir_df[non_vir_df['seq_len']>=100000]['seq_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_df.seq_len.plot(kind='box', rot=90, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_df[vir_df['seq_len']>=175000]['seq_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the box plots for the two classes of viruses, there are clearly outliers present in the data.  For virulent sequences, we will be later filtering around DNA sequence length of 50,000 nucleotides, and same for non-virulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample and concatenate two dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the outliers found above, we will take a sample from each dataframe, and then merge/concat the two objects into a single one to show the box plot again and test if the outliers are still the same.  We found out that there is still a thredshold around 25,000 and around 50,000.  For the virulent strains, those above 175,000 are Small Pox, while uncertain which species of virus for the non-virulent becuase there are over 300 of animal or plant viruses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_df_sample = vir_df.sample(n=10000, weights='seq_len', random_state=42)\n",
    "non_vir_df_sample = non_vir_df.sample(n=10000, weights='seq_len', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.concat([vir_df_sample, non_vir_df_sample])\n",
    "sample_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.seq_len.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([vir_df, non_vir_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = filterOutliers(merged_df)\n",
    "filtered_df = merged_df[merged_df.seq_len<=50000]\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.seq_len.plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct common protein analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next few cells are the first iteration on how we performed the protein analysis.  They take a few minutes to run each because there are over 40,000 sequences, of length in the hundreds to 50,000 nucleotides, so the conversion process is computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, translate the DNA nucleotide sequences to proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "filtered_df['p_seq'] = filtered_df['seq_str'].apply(lambda x: proteinTranslation(x))\n",
    "print(f'Took {time.time() - t1} seconds to translate nucleotides to proteins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates the GC content from DNA sequence of nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['gc_content'] = filtered_df['seq_str'].apply(lambda x: calcGcContent(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming the sequence columns to be more user friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.rename(columns={'p_seq': 'protein_sequence', 'seq_str': 'nucleotide_sequence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each sequence, run a protein analysis on the protein sequence to obtain metrics/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa, mw, a, ii = np.apply_along_axis(proteinAnalysis, 0, filtered_df['protein_sequence'].values)\n",
    "new_cols = ['amino_acid_percents','molecular_weights','aromaticity','instability_index']\n",
    "filtered_df[new_cols] = pd.DataFrame({'amino_acid_percents': aa, \n",
    "                                      'molecular_weights': mw, \n",
    "                                      'aromaticity': a, \n",
    "                                      'instability_index': ii})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the sequence ID column as they contain suffixes which won't be able to join metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['seq_id'] = filtered_df['seq_id'].apply(lambda x: x.replace('.1', '').replace('.2', '').replace('.3', '').replace('.4', '').replace('.5', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert numeric columns to float data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['molecular_weights', 'aromaticity', 'instability_index']\n",
    "filtered_df[num_cols] = filtered_df[num_cols].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.set_index('seq_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import metadata for pathogenic and nonpathogenic sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using the raw sequence data, we thought there might be something useful in the metadata of the tables in the NCBI Virus database.  So here we import the two files of same sequences (virulent and non-virulent) and then join the sequence data to the metadata on the index, which is also called the Accession ID.  Since the suffix to sequence ID was removed, the join will now be successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_meta_df = pd.read_csv('pathogenic_sequences_table.csv')\n",
    "non_vir_meta_df = pd.read_csv('non_pathogenic_sequences_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_meta_df.set_index('Accession', inplace=True)\n",
    "vir_meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_vir_meta_df.set_index('Accession', inplace=True)\n",
    "non_vir_meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.concat([vir_meta_df, non_vir_meta_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.merge(meta_df, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save point #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the state of this dataframe in CSV so that we can resume the analysis below without performing the above tasks over again, since they were somewhat computationally intensive to translate to proteins and run protein analysis on 40,000 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('virus_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect feature distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a save point where we can resume analysis in the notebook withouth having to repeat the protein translation and analysis steps.  This will just import the saved state as a new dataframe object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.read_csv('virus_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.drop('Isolation_Source', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a list of all the unique virus names found in the metadata for sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_names = analysis_df.sort_values(by='Species').Species.unique().tolist()\n",
    "virus_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the species names to lower case text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df['Species'] = analysis_df['Species'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the count of viruses by family name under genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_families = analysis_df.Family.unique().tolist()\n",
    "analysis_df.Family.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly, we can count the number of virulent vs. non-virulent in each family using crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(analysis_df['Family'], analysis_df['class']).sort_values(by='virulent', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function to iteratively loop through each virus name specified to see the sequence length distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSequenceLenHistogram(df, bins, strain):\n",
    "    df[(df['Species'].str.contains(strain))]['seq_len'].hist(bins=bins)\n",
    "    plt.title(f'{strain} Protein Length Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_names = ['ebola', 'hanta', 'severe acute respiratory', 'influenza', 'rabies', 'dengue', 'hepatitis', 'hepacivirus', 'rota', 'chikungunya', 'west nile', 'zika', 'measles', 'middle east']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in virus_names:\n",
    "    plotSequenceLenHistogram(analysis_df, 50, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a pairplot in seaborn to see if there are any correlations between the protein analysis metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=analysis_df[['seq_len', 'gc_content', 'molecular_weights', 'aromaticity', 'instability_index', 'class']], hue='class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THere aren't any clear correlations amonths these feature pairs, although sequence length does appear to have a role in distinguishing the two classes for molecular weights, aromaticity and instability index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hisogram of sequence length for virulent virus species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[(analysis_df['class']=='virulent')].seq_len.hist(bins=50)\n",
    "#  & (analysis_df['seq_desc'].str.contains('complete')) & (analysis_df['seq_len']>= 10000)\n",
    "plt.title('virulent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hisogram of sequence length for non-virulent virus species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[(analysis_df['class']=='non-virulent')].seq_len.hist(bins=50) \n",
    "#  & (analysis_df['seq_desc'].str.contains('complete')) & (analysis_df['seq_len'] <= 80000) & (analysis_df['seq_len']>= 10000)\n",
    "plt.title('non-virulent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the two histograms above, it is lear that the sequence length for non-virulent strains has a near normal distribution between 0 to 10,000 length with some \"outliers\" above 40,000.  The virulent strains however are very multimodal, and more sparse, or not consistently clustered in the same length range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below can be ignored because it was used to filter strains to complete and to specific sequence length ranges based on class, but we figured this might inject some bias in the sample used to train models below.  The viruses should be complete based on the downloaded files, and the entire dataset has already removed outliers of length >= 50,000 nucleotides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing where the virus sequences obtained from the database originate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df['Country'] = analysis_df['Geo_Location'].apply(lambda x: str(x).split(':')[0])\n",
    "analysis_df['Country'] = analysis_df['Country'].replace(np.nan, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = analysis_df['Country'].value_counts().reset_index()\n",
    "country_counts.columns = ['Country', 'Virus_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(x='Country', y='Virus_Count', data=country_counts.head(50), ax=ax)\n",
    "plt.title('Count of virus sequences in dataset by country')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_df = pd.concat([\n",
    "#     filtered_df[(filtered_df['class']=='virulent') & (filtered_df['seq_desc'].str.contains('complete')) & (filtered_df['seq_len']>= 10000)],\n",
    "#     filtered_df[(filtered_df['class']=='non-virulent') & (filtered_df['seq_desc'].str.contains('complete')) & (filtered_df['seq_len'] <= 80000) & (filtered_df['seq_len']>= 10000)]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing single quote with double quotes in amino acid percents column so that json object can be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df['amino_acid_percents'] = analysis_df['amino_acid_percents'].apply(lambda x: json.loads(x.replace(\"'\", '\"')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a column name for each amino acid to be parsed from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_list = list(amino_acids.values())\n",
    "aa_cols = ['amino_acid_' + aa for aa in aa_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse amino acid percent dictionary into a series, and perform lambda column renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids_percent_df = analysis_df['amino_acid_percents'].apply(pd.Series)\n",
    "amino_acids_percent_df = amino_acids_percent_df.rename(columns = lambda x : 'amino_acid_' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation heatmap of amino acid consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing mutual information as heatmap with values\n",
    "corr = amino_acids_percent_df.corr()\n",
    "plt.figure(figsize = (16,16))\n",
    "ax = sns.heatmap(corr,annot=True, linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this heatmap, we can see that multiple amino acid pairs have a high correlation.  For example, amino acid F and Y, as well as amino acids D with M and K, all have over .50 correlation R^2 value.  We're not subject matters on amino acids so not sure what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script for plotting kde with scatter obtained from seaborn documentation\n",
    "# https://seaborn.pydata.org/tutorial/distributions.html\n",
    "g = sns.jointplot(x=\"amino_acid_E\", y=\"amino_acid_Y\", data=amino_acids_percent_df, kind=\"kde\", color=\"m\")\n",
    "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")\n",
    "g.ax_joint.collections[0].set_alpha(0)\n",
    "g.set_axis_labels(\"$X$\", \"$Y$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are too many dense overlapping data points for this to be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KDE scattergram of comparing amino acid E and Y together, since this pair has .55 correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.kdeplot(amino_acids_percent_df[\"amino_acid_E\"], amino_acids_percent_df[\"amino_acid_Y\"], ax=ax)\n",
    "sns.rugplot(amino_acids_percent_df[\"amino_acid_E\"], color=\"g\", ax=ax)\n",
    "sns.rugplot(amino_acids_percent_df[\"amino_acid_Y\"], vertical=True, ax=ax)\n",
    "plt.title('KDE Plot of relationship between Amino Acids E and Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KDE plot helps to see the density and maybe some of the underlying correlation between these two features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging/concatenating the analyais dataframe with the amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.concat([analysis_df, amino_acids_percent_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TFIDF vectors to test for clustering in protein similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to see if there was a way to cluster the viruses using the genome sequence data, and not the protein analysis metrics first.  This was an initial attempted at unsupervised learning with this dataset.  When you see a protein sequence, it literally looks like a long string of letters from the alphabet.  Each letter represents an amino acid, and a chain of amino acids in between start and stop codons (triplets of nucleotides) creates a protein, but when the proteins are separated with spaces, it kind of looks like a sentence of words.  This gave me the idea of vectorizing the proteins using the TFIDF algorithm, and then performing cluster and similarity analysis with the \"syntax\" of relevant protein subsequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_corpus = analysis_df.protein_sequence.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the TFIDF vectorizer model and transform the corpus of proteins (array of protein sequences) into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 10), \n",
    "                             analyzer='word', norm='l2', \n",
    "                             max_features=5000, lowercase=False,\n",
    "                             min_df=.01, max_df=.90)\n",
    "protein_vectors = vectorizer.fit_transform(protein_corpus)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(f'Took {t2-t1:.3} seconds to build TFIDF vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_vectors.toarray()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the complete list of feature names, or protein subsequences from the vectorized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataframe object with selected columns and merge with TFIDF protein vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = analysis_df.reset_index().seq_id.values\n",
    "classes = analysis_df['class'].values\n",
    "species = analysis_df['Species'].values\n",
    "family = analysis_df['Family'].values\n",
    "country = analysis_df['Country'].values\n",
    "amino_acids = analysis_df[aa_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['seq_len', 'gc_content', 'molecular_weights', 'aromaticity', 'instability_index']\n",
    "numeric = analysis_df[num_cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = pd.DataFrame(protein_vectors.toarray())\n",
    "vector_df['ids'] = ids\n",
    "vector_df['class'] = classes\n",
    "vector_df['species'] = species\n",
    "vector_df['family'] = family\n",
    "vector_df['country'] = country\n",
    "vector_df[aa_cols] = pd.DataFrame(amino_acids)\n",
    "vector_df[num_cols] = pd.DataFrame(numeric_scaled)\n",
    "vector_df.set_index('ids', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.groupby(['class','family']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save point #2 post transformation to TFIDF vector, and merging with analysis dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_df.to_csv('analysis_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If continuing from running the code above, there is no need to run the next cell to create vector_df from the CSV file, as it's already in memory.  This save point is just to save time and jumping back into the analysis at a different part of the notebook workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_df = pd.read_csv('analysis_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_cols = list(np.arange(1000))\n",
    "num_cols = ['seq_len', 'gc_content', 'molecular_weights', 'aromaticity', 'instability_index']\n",
    "cat_cols = ['species', 'family', 'country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output CSV of aggregation on class family and species by average of each TFIDF vector feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_df.groupby(by=['class', 'family', 'species'])[vec_cols].mean().to_csv('class_vec_mean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform some unsupervised clustering on the virus dataset, we transformed the data using truncated SVD method.  This allowed us to reduce the dimensionality of the dataset down to just 5 features.  Although, this is still too many features to be represented on a 2D or 3D visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = vector_df.drop(['class', 'species', 'family', 'country'], axis=1)\n",
    "svd_df.set_index('ids', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd_features = np.concatenate((protein_vectors.toarray(), vector_df[num_cols].values), axis=1) # with numeric features\n",
    "svd_features = protein_vectors.toarray() # without numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    "# Create a function\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=30)\n",
    "X_tsvd = tsvd.fit(svd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_var_ratios = tsvd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_n_components(tsvd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=6, random_state=42)\n",
    "svd_matrix = svd.fit_transform(svd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd_range = np.arange(1,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tsvd_range, tsvd_var_ratios)\n",
    "plt.xlim(0,25)\n",
    "plt.title('Ratio of Explained Variance for each k components in SVD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should tell us that the ideal number of SVD components based on explained variance is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    svd_col = 'svd_' + str(i+1)\n",
    "    vector_df[svd_col] = svd_matrix[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation that the SVD features are added to the vector dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use an algorithm like Kmeans to see if there is potential for clustering the viruses based on the engineered features from protein vectors and protein analysis metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = []\n",
    "\n",
    "for i in range(1, 40):\n",
    "    print(f'Building kmeans with {i+1} clusters')\n",
    "    kmeans = KMeans(n_clusters=i+1)\n",
    "    kmeans.fit(svd_matrix)\n",
    "    print(f'Current sum of squared distance: {kmeans.inertia_:.2}')\n",
    "    ssd.append(kmeans.inertia_)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting to find the number of clusters\n",
    "plt.plot(range(1,40), ssd,'bx-')\n",
    "plt.xlabel(\"No. of clusters\")\n",
    "plt.ylabel(\"Sum of Squared Distance\") #sum of squares within clusters\n",
    "plt.title(\"The Elbow Method showing the optimal Kmeans clusters\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow method for KMeans inertia (sum of squared distance between instances) we can see that even though there is a distinguishable change in slope between K = 10 and K = 20, the sum of squared distance is still at a very high scale.  Usually the SSD would be significantly lower.  At this xcale, the required number of clusters to reduce the SSD to a reasonable level means we would need maybe hundreds of clusters.  Therefore this method won't yield good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit(vector_df.drop(['ids','class','species','family','country'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df['cluster'] = kmeans.predict(vector_df.drop(['ids','class','species','family','country'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 40\n",
    "k_range = range(1, k_max+1)\n",
    "\n",
    "# Cluster data into k=1..k_max clusters.\n",
    "ests = [KMeans(n_clusters=k).fit(svd_matrix) for k in k_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dists(X, centroids):\n",
    "    \"\"\"Calculate distance of each instance from nearest centroid.\"\"\"\n",
    "    dists = [cdist(X, center, 'euclidean') for center in centroids]\n",
    "    dist = [np.min(dist, axis=1) for dist in dists]\n",
    "    c_idx = [np.argmin(dist, axis=1) for dist in dists]\n",
    "    \n",
    "    return dist, c_idx\n",
    "\n",
    "\n",
    "def get_metrics(X, dist):\n",
    "    \"\"\"Cluster \"goodness\" metrics.\"\"\"\n",
    "    total_within_ss = [sum(d**2) for d in dist]  # total within-cluster sum of squares\n",
    "    total_ss = sum(pdist(X)**2) / X.shape[0]     # total sum of squares\n",
    "    between_ss = total_ss - total_within_ss      # between-cluster sum of squares\n",
    "    \n",
    "    return total_ss, between_ss\n",
    "\n",
    "\n",
    "def plot_elbow(k_range, total_ss, between_ss, n_true=None):\n",
    "    \"\"\"Plot elbow curve.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    var_exps = (between_ss / total_ss) * 100\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(k_range, var_exps, 'b*-')\n",
    "\n",
    "    if n_true is not None:\n",
    "        # Add marker for ground truth.\n",
    "        kwargs = {'marker': 'o', 'markersize': 12, 'markeredgewidth': 2,\n",
    "                  'markeredgecolor': 'r', 'markerfacecolor': 'None'}\n",
    "        k_true = k_range[n_true-1]\n",
    "        k_true_var_exp = (between_ss[n_true-1] / total_ss) * 100\n",
    "        ax.plot(k_true, k_true_var_exp, **kwargs)\n",
    "\n",
    "    ax.set_xlim((0, k_range[-1]))\n",
    "    ax.set_ylim((0, 100))\n",
    "    ax.set_xticks(k_range)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Variance Explained (%)\")\n",
    "    plt.title(\"k-Means Clustering Elbow Plot\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = [est.cluster_centers_ for est in ests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: calculating the distance from centroids might take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, c_idx = get_dists(svd_matrix, centroids)\n",
    "total_ss, between_ss = get_metrics(svd_matrix, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow(k_range, total_ss, between_ss, n_true=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot of truncated SVD components with cluster, class and species, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(vector_df.reset_index(), x=\"svd_1\", y=\"svd_2\", z='svd_3', color=\"class\", opacity=.25,\n",
    "                 hover_data=['class', 'ids', 'species', 'family', 'country', 'cluster'], color_continuous_scale=px.colors.sequential.Viridis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(vector_df.reset_index(), x=\"svd_1\", y=\"svd_2\", z='svd_3', color=\"cluster\", opacity=.25,\n",
    "                 hover_data=['class', 'ids', 'species', 'family', 'country', 'cluster'], color_continuous_scale=px.colors.sequential.Viridis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly with 20 custers, the virus sequences are separated pretty well.  For exampe, Middle East Respiratory Syndrome and Severe Accute Respiratory Syndrom are near eachother, but labeled with two different clusters.  So even in a small dimensional area, the clustering is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at Dendrogram for Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    children = model.children_\n",
    "    dist = np.arange(children.shape[0])+1\n",
    "    n_obs = np.arange(2, children.shape[0]+2)\n",
    "    linkage_matrix = np.column_stack([children, dist, n_obs])\n",
    "    dendrogram(linkage_matrix.astype(float), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_family = vector_df['family'].nunique()\n",
    "n_family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vector_df[['seq_len', 'gc_content', 'molecular_weights', 'aromaticity', 'instability_index','cluster','class']].copy()\n",
    "classes = X_vect.pop('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AgglomerativeClustering(n_clusters=n_family)\n",
    "# model = model.fit(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = plt.figure(figsize=(12, 20))\n",
    "# labels = vector_df['species'].values\n",
    "# plt.title('Hierarchical Clustering Dendrogram of Virus RNA')\n",
    "# plot_dendrogram(model, labels=labels, orientation='right', leaf_font_size=9)\n",
    "# plt.savefig('species_svd_dendrogram.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were unsuccessful at creating a dendrogram with the full dataset or even a scaled down one using a selection of the SVD components generated from the protein vectors with the protein analysis metrics.  The agglomerative clustering fitting and dendrogram plotting process takes too long, so we've had to cancel the process many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster map from Kmeans cluster and protein analysis features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Running the clustermap visualization might take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "lut = dict(zip(classes.unique(), \"rbg\"))\n",
    "row_colors = classes.map(lut)\n",
    "g = sns.clustermap(X_vect, row_colors=row_colors)\n",
    "plt.title('Dendrogram Clustermap of Protein metrics with Kmeans Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use simple OLS linear regression and LogisticRegression as baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_cols = list(np.arange(5000))\n",
    "vec_cols_str = [str(col) for col in vec_cols]\n",
    "vec_col_str = ' + '.join(vec_cols_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_cols = ['svd_' + str(i+1) for i in np.arange(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.matrix(vector_df[vec_cols_str].values)\n",
    "y = np.array(vector_df[target_col].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = []\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'virulent':\n",
    "        y_bin.append(1)\n",
    "    else:\n",
    "        y_bin.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: running the Ordinary Least Squares (OLS) model takes a few minutes to print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_bin, x).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_df = pd.DataFrame({'vector': vec_cols, 'pval': model.pvalues, 'coef': model.params})\n",
    "pval_df['proteins'] = vectorizer.get_feature_names()\n",
    "# pval_df[pval_df['pval'] <= .05].to_csv('protein_importance.csv', index=False)\n",
    "pval_df.to_csv('protein_importance_unfiltered.csv', index=False)\n",
    "pval_list = pval_df[pval_df['pval'] <= .05]['vector'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Positions of 25 out of 102 most relevant/important proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_list[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the OLS model, there are 102 protein vectors which have a statistically significant relationship with the predictive outcome for virulent class.  However, OLS isn't optimized for a binary nor multiclass classification problem.  However, this might still provide useful insight into the feature importance for training models more appropriate for classification.  Most of the relevant protein vectors are very short, only 2-3 protein characters long, however some are longer.  For example:\n",
    "\n",
    "EAVGINWSVHQHHATFSPLPNHLMFMSYCSSLQAVPWVALGHGH\n",
    "\n",
    "This is a protein associated with Hepatits B virus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = analysis_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df.to_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save point #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df = pd.read_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_cols = list(np.arange(0,5000)) # vec_cols + \n",
    "cols_to_drop = ['seq_id','seq_name','seq_desc','nucleotide_sequence','protein_sequence','class','Country','Genus','Species','Family','Length','Nuc_Completeness','Geo_Location','Host','Collection_Date','Release_Date','amino_acid_percents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['class'] = model_df['class'] == 'virulent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.drop(cols_to_drop, axis=1).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_w_class = [col for col in cols_to_drop if col != 'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation heatmap of model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_corr = model_df.drop(cols_to_drop_w_class, axis=1).corr()\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(model_corr, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataframe of features for predictive model by random 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(model_df.drop(cols_to_drop, axis=1), model_df['class'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Guassian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_nb = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred_nb, labels=[False, True]), annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Extra Trees classifier, the Naive Bayes model creates over 5,000 false negatives and over 600 false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_nb, labels=[False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression()\n",
    "lrc.fit(X_train, y_train)\n",
    "y_pred_lr = lrc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr, labels=[False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Relevant Features for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the logistic regression model performed well, but not the best (significantly better than NB), we wanted to try using a feature selection model to narrow the feature set to optimize the logistic regression model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFE(lrc, n_features_to_select=1)\n",
    "selector = selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy['class_label'] = Xy['class'].apply(lambda x: 'virulent' if x else 'non-virulent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation heatmap of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_corr = Xy.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(Xy_corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the correlation heatmap of just the training data, only a few features have a higher correlation coefficient with the target variable 'class'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a mask of features to keep based on logistic regression classifier coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.round(lrc.coef_, decimals=2)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, val in enumerate(mask[0]):\n",
    "    print(ii, val)\n",
    "    if val == True:\n",
    "        cols_to_keep.append(X_train.columns.tolist()[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cols_to_keep].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to split the data on features kept from logistic regression coefficient values > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(model_df[cols_to_keep], model_df['class'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc2 = LogisticRegression()\n",
    "lrc2.fit(X_train2, y_train2)\n",
    "y_pred_lr2 = lrc2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test2, y_pred_lr2), annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc2.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test2, y_pred_lr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the above that filtering the feature set based on coefficient > 0 didn't help improve the performance of the logistic regression model at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Ensemble Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, the random forest ensemble performs much better than logistic regression or gaussian naive bayes.  Compared to the extra trees ensemble below, though, they perform very similarly, there is only 0.002 difference in accuracy, and the confusion matrices look almost the same depending on training and run version.  The major difference is that random forest takes a little longer to train, so performance wise, the extra trees is better for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred_rf = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf, labels=[False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Extra Trees Ensemble Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see below, the extra trees classifier easily out performs the other vanilla classifiers without any kin dof hyperparameter tuning of the models.  This ensemble achieves near 100% accuracy, and very minimal Type I or Type II errors.  However, we are both skeptic that this is overfitting on multicolliearity of sequence length and molecular weight, as well as potential bias or limitations in the dataset downloaded from the NCBI Virus database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier()\n",
    "etc.fit(X_train, y_train)\n",
    "y_pred_et = etc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, y_pred_et), annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there are only 8 false positives and 12 false negatives.  Very highly performant model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(etc, X_test, y_test)  # doctest: +SKIP\n",
    "plt.grid(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Near 100% accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_et, labels=[False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating what the most important features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = etc.estimators_\n",
    "importance = etc.feature_importances_\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importance)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importance[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in indices:\n",
    "    train_cols.append(X_train.columns[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame({\n",
    "    'index': indices,\n",
    "    'feature': X_train.columns.tolist(),\n",
    "    'importance': importance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.sort_values(by='importance', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.plot.bar(x='feature', y='importance', rot=90, figsize=(10,5))\n",
    "plt.title('Feature Importance (Extra Trees Classifier)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = etc.feature_importances_\n",
    "feats = {}\n",
    "# summarize feature importance\n",
    "for i,v in zip(X_train.columns, importance):\n",
    "    feats[i] = v\n",
    "    print(i,'Score: %.5f' % (v))\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances.sort_values(by='Gini-importance').plot(kind='bar', rot=90, figsize=(10,10), title = \"Feature Importance (Extra Tree Classifier)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_curve(lg, X,y):\n",
    "    # instantiate\n",
    "\n",
    "    # fit\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "        \n",
    "    x1 = np.linspace(0, 10, 8, endpoint=True) produces\n",
    "        8 evenly spaced points in the range 0 to 10\n",
    "    \"\"\"\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(lg, X, y, cv = 5,n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"ExtraTreeClassifier\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # box-like grid\n",
    "    plt.grid()\n",
    "    \n",
    "    # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # plot the average training and test score lines at each training set size\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # sizes the window for readability and displays the plot\n",
    "    # shows error from 0 to 1.1\n",
    "    plt.ylim(-.1,1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(etc,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A precision-recall curve shows the relationship between precision (= positive predictive value) and recall (= sensitivity) for every possible cut-off.\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "disp = plot_precision_recall_curve(etc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = X_test['seq_len']\n",
    "weights = X_test['molecular_weights']\n",
    "gc = X_test['gc_content']\n",
    "eaa = X_test['amino_acid_E']\n",
    "paa = X_test['amino_acid_P']\n",
    "actual = y_test.values\n",
    "predicted = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = pd.DataFrame({\n",
    "    'length': length,\n",
    "    'weight': weights,\n",
    "    'gc_content': gc,\n",
    "    'amino_acid_e': eaa,\n",
    "    'amino_acid_p': paa,\n",
    "    'actual': actual,\n",
    "    'predicted': predicted\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['weight','gc_content','amino_acid_e','amino_acid_p']:\n",
    "    g = sns.FacetGrid(model_out, col='actual',  hue=\"predicted\", hue_order=[True, False])\n",
    "    g = (g.map(plt.scatter, 'length', col, edgecolor=\"w\")\n",
    "          .add_legend())\n",
    "    plt.title(f'Comparison of Truth to Prediction on Length to {col}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(model_out, x=\"weight\", y=\"amino_acid_e\", z=\"gc_content\", color=\"predicted\", opacity=.25,\n",
    "                 hover_data=['length','weight','gc_content','amino_acid_e','actual','predicted'], color_continuous_scale=px.colors.sequential.Viridis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Extra Trees Classifier Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the full decision tree diagram below for the Extra Trees Classifier, it's difficult to make out the decisions at nodes at this resolution, but if you view the .PNG output file, you can scroll around to see how the model is making decisions at each node.  The first split is at Amino Acid H, then the next tier splits at Molecular Weights, and Amino Acid F, and so on down the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=['seq_len', 'gc_content', 'molecular_weights', 'aromaticity',\n",
    "       'instability_index', 'amino_acid_A', 'amino_acid_C', 'amino_acid_D',\n",
    "       'amino_acid_E', 'amino_acid_F', 'amino_acid_G', 'amino_acid_H',\n",
    "       'amino_acid_I', 'amino_acid_K', 'amino_acid_L', 'amino_acid_M',\n",
    "       'amino_acid_N', 'amino_acid_P', 'amino_acid_Q', 'amino_acid_R',\n",
    "       'amino_acid_S', 'amino_acid_T', 'amino_acid_V', 'amino_acid_W',\n",
    "       'amino_acid_Y']\n",
    "cn=['virulent','non-virulent']\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (20,20), dpi=800)\n",
    "tree.plot_tree(etc.estimators_[0],\n",
    "               feature_names = fn,\n",
    "               class_names=cn,\n",
    "               filled = True);\n",
    "fig.savefig('individualtree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict probabilities for the test data.\n",
    "y_proba = etc.predict_proba(X_test)\n",
    "#Keep Probabilities of the positive class only.\n",
    "y_proba = y_proba[:, 1]\n",
    "#Compute the AUC Score.\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba, pos_label=1)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance with AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(X, y, models, model_names):\n",
    "    plt.figure(0, figsize = [8, 7]).clf()\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([-.01, 1.01])\n",
    "    plt.ylim([-.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    \n",
    "    for ii, model in enumerate(models):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        fpr, tpr, threshold = roc_curve(y, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label = \"{} AUC = {:0.2f}\".format(model_names[ii], roc_auc))\n",
    "\n",
    "    plt.legend(loc = 'lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(X_test, y_test, [etc, rfc, lrc, gnb], ['extra trees', 'random forest', 'logistic regression','gaussian naive bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we achieved some interesting results with clustering tools, but ultimately some of the simplest features obtained from the raw data and protein analysis yielded the best predictive results if a virus is virulent or not.  Clearly the ensemble Extra Trees model worked the best of the three models attempted here.  Perhaps others would have performed just as well or in between the gap but achieving near 100% accuracy and minimal Type I and Type II errors tells us that the Extra Trees ensemble is ready for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing ExtraTreesClassifier model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(etc, 'virus_extra_trees_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
