{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import default madules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from builtins import range\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# import geographic modules\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import show_hist\n",
    "from rasterio.mask import mask\n",
    "from affine import Affine\n",
    "import mercantile\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.vrt import WarpedVRT\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "from fiona.crs import from_epsg\n",
    "import pycrs\n",
    "from osgeo import ogr, gdal\n",
    "import geojson\n",
    "from geojson import Polygon, MultiPolygon\n",
    "from matplotlib.pyplot import figure\n",
    "import geopandas as gpd\n",
    "import geoplot\n",
    "import fiona\n",
    "import pprint\n",
    "import shapely\n",
    "import pyproj\n",
    "from PIL import Image\n",
    "from skimage import data\n",
    "from skimage.util.dtype import dtype_range\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import exposure\n",
    "from skimage.morphology import disk\n",
    "from skimage.filters import rank\n",
    "\n",
    "# import keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import utils\n",
    "from keras import callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.callbacks.callbacks import LearningRateSchedulerPerBatch\n",
    "from keras import backend as K\n",
    "\n",
    "line_filler = '-'*75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build path for relevant files from each country subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [entry.path for entry in os.scandir('stac') if entry.is_dir()]\n",
    "all_dirs = []\n",
    "\n",
    "for a_dir in dirs:\n",
    "    sub_dirs = [entry.path for entry in os.scandir(a_dir) if entry.is_dir()]\n",
    "    for pathy in sub_dirs:\n",
    "        all_dirs.append(pathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_train_files = []\n",
    "geojson_test_files = []\n",
    "tif_image_files = []\n",
    "list_of_tif_geojson_files = []\n",
    "\n",
    "for dir_path in all_dirs:\n",
    "    this_dict = {}\n",
    "    this_path = Path(dir_path)\n",
    "    locale = this_path.parts[-1]\n",
    "    print(f'Current path is {this_path}')\n",
    "    print(f'Locale of the current path {locale}')\n",
    "    files = os.listdir(this_path)\n",
    "    for file in files:\n",
    "        this_dict['batch_path'] = this_path\n",
    "        if file == 'train-' + locale + '.geojson':\n",
    "            geojson_train_files.append(os.path.join(this_path, file))\n",
    "            this_dict['train_file'] = file\n",
    "        elif file == 'test-' + locale + '.geojson':\n",
    "            geojson_test_files.append(os.path.join(this_path, file))\n",
    "            this_dict['test_file'] = file\n",
    "        elif '.tif' in file and '4326' in file:\n",
    "            tif_image_files.append(os.path.join(this_path, file))\n",
    "            this_dict['tif_file'] = file\n",
    "        else:\n",
    "            pass\n",
    "    list_of_tif_geojson_files.append(this_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tif_geojson_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_geojson_df = pd.DataFrame(list_of_tif_geojson_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_geojson_df = tif_geojson_df.dropna()\n",
    "tif_geojson_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_geojson_df['batch_path'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_image_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_train_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_test_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training geojson files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(geojson_train_files[0]) as data_file:    \n",
    "    json_data = geojson.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geom = [json_data[0]['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "\n",
    "for feature in json_data['features']:\n",
    "    this_dict = {}\n",
    "    this_dict['coordinates'] = feature['geometry']\n",
    "    this_dict['id'] = feature['properties']['id']\n",
    "    this_dict['material'] = feature['properties']['roof_material']\n",
    "    this_dict['verified'] = feature['properties']['verified']\n",
    "    feature_list.append(this_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['polygons'] = feature_df['coordinates'].apply(lambda x: len(x['coordinates'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = gdal.Open(tif_image_files[0])\n",
    "gt = ds.GetGeoTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ds.RasterCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = ds.GetRasterBand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = band.ReadAsArray(0, 0, image_file.width, image_file.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_with_geoms(geom_array, image_file):\n",
    "    out_img, out_transform = mask(raster=image_file, shapes=coords, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(gdf):\n",
    "    \"\"\"Function to parse features from GeoDataFrame in such a manner that rasterio wants them\"\"\"\n",
    "    return [json.loads(gdf.to_json())['features'][0]['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = rasterio.open(tif_image_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(image_file)\n",
    "# show((image_file, 1), cmap='terrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(image_file.crs)\n",
    "print(image_file.transform)\n",
    "print(image_file.width)\n",
    "print(image_file.height)\n",
    "print(image_file.bounds)\n",
    "print(image_file.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproj_image_file = rasterio.open('stac\\\\colombia\\\\borde_rural\\\\borde_rural_ortho_cog-epsg4326_reproj.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reproj_image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reproj_image_file.crs)\n",
    "print(reproj_image_file.transform)\n",
    "print(reproj_image_file.width)\n",
    "print(reproj_image_file.height)\n",
    "print(reproj_image_file.bounds)\n",
    "print(reproj_image_file.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproj_image_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# figure(num=None, figsize=(, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "fig = plt.figure(figsize=(26,42))\n",
    "reproj_thumb = reproj_image_file.read(1,  out_shape=(1, int(reproj_image_file.height // 12), int(reproj_image_file.width // 12)))\n",
    "plt.imshow(reproj_thumb)\n",
    "plt.colorbar()\n",
    "plt.title(f'Minimum resolution band {thumb.shape}\\n')\n",
    "plt.xlabel('Column #')\n",
    "plt.ylabel('Row #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = image_file.overviews(1)\n",
    "least_v = overview[-2]\n",
    "thumb = image_file.read(1, out_shape=(1, int(image_file.height // least_v), int(image_file.width // least_v)))\n",
    "print(thumb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,45))\n",
    "plt.imshow(thumb)\n",
    "plt.colorbar()\n",
    "plt.title(f'Minimum resolution band {thumb.shape}\\n')\n",
    "plt.xlabel('Column #')\n",
    "plt.ylabel('Row #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "buildings = gpd.read_file(geojson_train_files[0])\n",
    "print(buildings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_train_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoplot.polyplot(buildings, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(gdf_json, i):\n",
    "    \"\"\"Function to parse features from GeoDataFrame in such a manner that rasterio wants them\"\"\"\n",
    "    return [gdf_json['features'][i]['geometry']]\n",
    "\n",
    "def bounding_box(points):\n",
    "    try:\n",
    "        x_coords, y_coords = [], []\n",
    "        for i in range(len(points)):\n",
    "            row_x, row_y = zip(building_coords[0]['coordinates'][0][i])\n",
    "            x_coords.append(row_x[0])\n",
    "            y_coords.append(row_y[0])\n",
    "    #     x_coordinates, y_coordinates = zip(points)\n",
    "        return [(min(x_coords), min(y_coords)), (max(x_coords), max(y_coords))]\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_json = json.loads(buildings.to_json())\n",
    "building_coords = get_features(buildings_json, 0)\n",
    "# type(buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(buildings_json['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tif_geojson_df)):\n",
    "    print(tif_geojson_df.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract building .tif files from highres tif using geojson coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(len(tif_geojson_df.dropna())):\n",
    "        batch_path = tif_geojson_df['batch_path'].iloc[i]\n",
    "        print(f'Processing file batch in path {batch_path}')\n",
    "        \n",
    "        tif_file = os.path.join(batch_path, tif_geojson_df['tif_file'].iloc[i])\n",
    "        print(f'Tiff filename: {tif_file}')\n",
    "        reproj_image_file = rasterio.open(tif_file)\n",
    "        \n",
    "        print(reproj_image_file.crs)\n",
    "        print(reproj_image_file.transform)\n",
    "        print(reproj_image_file.width)\n",
    "        print(reproj_image_file.height)\n",
    "        print(reproj_image_file.bounds)\n",
    "        print(reproj_image_file.meta)\n",
    "        \n",
    "        train_json = os.path.join(batch_path, tif_geojson_df['train_file'].iloc[i])\n",
    "        print(f'Training GeoJson filename: {train_json}')\n",
    "        \n",
    "        test_json = os.path.join(batch_path, tif_geojson_df['test_file'].iloc[i])\n",
    "        print(f'Testing GeoJson filename: {test_json}\\n')\n",
    "\n",
    "        for json_group in [[train_json, 'train'], [test_json, 'test']]:\n",
    "            model_path = json_group[1]\n",
    "            print(f'Processing {model_path}ing files')\n",
    "            buildings = gpd.read_file(json_group[0])\n",
    "            buildings_json = json.loads(buildings.to_json())\n",
    "\n",
    "            for i in range(len(buildings_json['features'])):\n",
    "                building_id = buildings_json['features'][i]['properties']['id']\n",
    "                if model_path == 'train':\n",
    "                    building_material = buildings_json['features'][i]['properties']['roof_material']\n",
    "                else:\n",
    "                    building_material = ''\n",
    "                print(f'Object id {building_id} has material type {building_material}')\n",
    "\n",
    "                building_coords = get_features(buildings_json, i)\n",
    "                print(f'Building origin coordinates:\\n {building_coords}')\n",
    "                box_coords = bounding_box(building_coords[0]['coordinates'][0])\n",
    "                \n",
    "                if box_coords:\n",
    "                    bbox = box(box_coords[0][0], box_coords[0][1], box_coords[1][0], box_coords[1][1])\n",
    "                    print(f'Object id {building_id} has boudning box:\\n {bbox}')\n",
    "\n",
    "                    geo_df = gpd.GeoDataFrame({'geometry': bbox}, index=[0], crs=reproj_image_file.crs.data)\n",
    "    #                 building_geo_coords = get_features(json.loads(geo.to_json()), i)\n",
    "\n",
    "                    out_arr, out_transform = mask(reproj_image_file, shapes=building_coords, crop=True)\n",
    "                    print('These are the attributes of the masked image space:')\n",
    "                    print(type(out_arr), out_arr.shape, out_arr.dtype, out_transform)\n",
    "\n",
    "                    out_meta = reproj_image_file.meta.copy()\n",
    "                    out_meta.update({\"driver\": \"GTiff\",\n",
    "                                    \"height\": out_arr.shape[1],\n",
    "                                     \"width\": out_arr.shape[2],\n",
    "                                     \"transform\": out_transform})\n",
    "\n",
    "                    temp_file_name = building_id + '.tif'\n",
    "\n",
    "                    if model_path == 'train':\n",
    "                        folder_path = os.path.join(model_path, building_material)\n",
    "                    else:\n",
    "                        folder_path = model_path\n",
    "\n",
    "                    if not os.path.exists(folder_path):\n",
    "                        os.makedirs(folder_path)\n",
    "\n",
    "                    file_path = os.path.join(folder_path, temp_file_name)\n",
    "                    \n",
    "                    if os.path.exists(file_path):\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(f'Writing clipped image file to: {folder_path}')\n",
    "                        with rasterio.open(file_path, 'w', **out_meta) as dest:\n",
    "                            dest.write(out_arr)\n",
    "                else:\n",
    "                    print(f'No coordinate bounds found for building {building_id}')\n",
    "                \n",
    "                print(line_filler, '\\n')\n",
    "except Exception:\n",
    "        print(line_filler)\n",
    "        print(traceback.format_exc())\n",
    "        print(line_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_json['features'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_matrix = building_coords[0]['coordinates'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_x, row_y = zip(building_coords[0]['coordinates'][0][0])\n",
    "print(row_x, row_y)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_type = buildings_json['features'][0]['properties']['roof_material']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectid = buildings_json['features'][0]['properties']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_coords[0]['coordinates'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_coord = bounding_box(building_coords[0]['coordinates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_coord = bounding_box(building_coords[0]['coordinates'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = box(box_coord[0][0], box_coord[0][1], box_coord[1][0], box_coord[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = gpd.GeoDataFrame({'geometry': bbox}, index=[0], crs=reproj_image_file.crs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = get_features(json.loads(geo.to_json()), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "geojson = fiona.open(geojson_train_files[0], \"r\")\n",
    "\n",
    "with fiona.open(geojson_train_files[0], \"r\") as geojson:\n",
    "    features = [feature[\"geometry\"] for feature in geojson]\n",
    "    pprint.pprint(features[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for coord_pair in features[0]['coordinates'][0]:\n",
    "    print(coord_pair)\n",
    "    x.append(coord_pair[0])\n",
    "    y.append(coord_pair[1])\n",
    "# for feature in features[0]['coordinates'][0]:\n",
    "#     print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = tif_image_files[0]\n",
    "in_file = in_file.split('\\\\')[-1]\n",
    "out_file = in_file.replace('cog', 'cog_wgs84_reproj')\n",
    "print(in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproj_image_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr, out_transform = mask(reproj_image_file, shapes=coords, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr.shape, out_arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_meta = reproj_image_file.meta.copy()\n",
    "out_meta.update({\"driver\": \"GTiff\",\n",
    "                \"height\": out_arr.shape[1],\n",
    "                 \"width\": out_arr.shape[2],\n",
    "                 \"transform\": out_transform})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'{objectid}.tif'\n",
    "\n",
    "with rasterio.open(f'{out_file}', 'w', **out_meta) as dest:\n",
    "    dest.write(out_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing looking at a single file, and some filtering options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'train//concrete_cement//7a1c6d7c_concrete_cement.tif'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = plt.imread(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Shape of old array: {img_arr.shape}')\n",
    "\n",
    "# image_width = img_arr.shape[0]\n",
    "# image_length = img_arr.shape[1]\n",
    "# image_layers = img_arr.shape[2]\n",
    "\n",
    "# img_arr_median = np.zeros((image_width, image_length, image_layers))\n",
    "# print(f'Shape of new array: {img_arr_median.shape}')\n",
    "\n",
    "# img_arr_median\n",
    "\n",
    "# for l in range(image_layers-1):\n",
    "#     layer_median_pixel_val = np.median(img_arr[:,:,l])\n",
    "#     print(f'Median pixel value on layer {l+1} is {layer_median_pixel_val}')\n",
    "    \n",
    "#     for i in range(image_width):\n",
    "#         for j in range(image_length):\n",
    "#             print(f'Before: {img_arr[i,j,l]}')\n",
    "#             if img_arr[i,j,l] - layer_median_pixel_val < 0:\n",
    "#                 img_arr_median[i,j,l] = 0\n",
    "#             else:\n",
    "#                 img_arr_median[i,j,l] = img_arr[i,j,l] - layer_median_pixel_val\n",
    "#             print(f'After: {img_arr_median[i,j,l]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr[:,:,0:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_arr[:,:,0:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_arr), img_arr.shape, img_arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2, p98 = np.percentile(img_arr, (2,98))\n",
    "img_arr_rescale_intensity = exposure.rescale_intensity(img_arr, in_range=(p2,p98))\n",
    "plt.imshow(img_arr_rescale_intensity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr_rescale = exposure.equalize_hist(img_arr)\n",
    "plt.imshow(img_arr_rescale)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = disk(30)\n",
    "img_arr_adapt_equal = exposure.equalize_adapthist(img_arr, clip_limit = .03)\n",
    "plt.imshow(img_arr_adapt_equal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buildig an array with all the original images to split into train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell was strictly to rename the .tif files to remove material type, leaving just building id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in train_paths:\n",
    "#     for file in os.listdir(path):\n",
    "#         print(file)\n",
    "#         new_file_name = file.split('_')[0] + '.tif'\n",
    "#         print(new_file_name)\n",
    "        \n",
    "#         old_path_file = os.path.join(path, file)\n",
    "#         new_path_file = os.path.join(path, new_file_name)\n",
    "        \n",
    "#         os.rename(old_path_file, new_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "image = plt.imread(test_path)\n",
    "size = 150, 150\n",
    "image_resized = skimage.transform.resize(image, size)\n",
    "plt.imshow(image_resized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train_path = 'train/'\n",
    "material_folders = [sub_path for sub_path in os.listdir('train')]\n",
    "material_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict_list = []\n",
    "y_train_dict_list = []\n",
    "size = 150, 150\n",
    "\n",
    "for material_type in material_folders:\n",
    "    sub_train_path = os.path.join(root_train_path, material_type)\n",
    "    for img_file in os.listdir(sub_train_path):\n",
    "        file_path = os.path.join(sub_train_path, img_file)\n",
    "        img = plt.imread(file_path)\n",
    "        img_resize = skimage.transform.resize(image, size)\n",
    "        img_arr = img_resize[:,:,0:3]\n",
    "        building_id = img_file.replace('.tif','')\n",
    "        \n",
    "        img_dict = {\n",
    "            'building_id': building_id,\n",
    "            'material_type': material_type,\n",
    "            'image_data': img_arr\n",
    "        }\n",
    "        x_train_dict_list.append(img_dict)\n",
    "        \n",
    "        label_dict = {\n",
    "            'building_id': building_id,\n",
    "            'concrete_cement': 0.0, \n",
    "            'healthy_metal': 0.0, \n",
    "            'incomplete': 0.0, \n",
    "            'irregular_metal': 0.0, \n",
    "            'other': 0.0\n",
    "        }\n",
    "        label_dict[material_type] = 1.0\n",
    "        y_train_dict_list.append(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict_list[0]['image_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array = []\n",
    "\n",
    "for i in range(len(x_train_dict_list)):\n",
    "    print(f'Appending index {i}')\n",
    "    x_train_array.append(x_train_dict_list[i]['image_data'])\n",
    "    \n",
    "x_train_array = np.array(x_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = []\n",
    "\n",
    "for i in range(len(y_train_dict_list)):\n",
    "    print(f'Appending index {i}')\n",
    "    material_val_list = [y_train_dict_list[i][key] for key in material_folders]\n",
    "    y_train_array.append(material_val_list)\n",
    "    \n",
    "y_train_array = np.array(y_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = pd.DataFrame(y_train_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = x_train_df['image_data'].values\n",
    "# y = y_train_df[material_folders].values\n",
    "X = x_train_array\n",
    "y = y_train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y[:,0]), np.sum(y[:,1]), np.sum(y[:,2]), np.sum(y[:,3]), np.sum(y[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# np.sum(y_res[:,0]), np.sum(y_res[:,1]), np.sum(y_res[:,2]), np.sum(y_res[:,3]), np.sum(y_res[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_train[:,0]), np.sum(y_train[:,1]), np.sum(y_train[:,2]), np.sum(y_train[:,3]), np.sum(y_train[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes =  5\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate a simple convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# num_classes = 10\n",
    "# epochs = 100\n",
    "# data_augmentation = True\n",
    "# num_predictions = 20\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# # The data, split between train and test sets:\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                  input_shape=X_train.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), input_shape=X_train.shape[1:], padding='same',\n",
    "           activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(),\n",
    "#     Dense(4096, activation='relu'),\n",
    "#     Dense(4096, activation='relu'),\n",
    "#     Dense(1000, activation='softmax'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 5\n",
    "epochs = 50\n",
    "model_save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_trained_vvg16.h5'\n",
    "\n",
    "# weight_file_name=\"weights-improvement-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "# weight_file_path = os.path.join(model_save_dir, weight_file_name)\n",
    "# checkpoint = ModelCheckpoint(weight_file_path, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=45,\n",
    "        samplewise_center=True,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size)\n",
    "#         class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        batch_size=batch_size)\n",
    "#         class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_callback = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss')\n",
    "# lr_decay_callback = LearningRateSchedulerPerBatch(\n",
    "#                     lambda step: ((learning_rate - min_learning_rate) * decay_rate ** step + min_learning_rate))\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',\n",
    "                          min_delta = 0,\n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "callbacks = [ckpt_callback, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "model_path = os.path.join(model_save_dir, model_name)\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size,\n",
    "        callbacks=callbacks\n",
    "        )\n",
    "model.save_weights(os.path.join(model_save_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "model_path = os.path.join(model_save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extractc_features(directory, sampple_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150,150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='category')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i*batch_size: (i+1) * batch_size] = features_batch\n",
    "        labels[i*batch_size: (i+1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features('train/concrete_cement', 2000)\n",
    "validation_features, validation_labels = extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras import optimizers, models, layers\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "print(conv_base.summary())\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=20, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I don't know what's goingon below here :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# in_file = in_file.split('\\\\')[-1]\n",
    "# out_file = in_file.replace('cog', 'cog_wgs84_reproj')\n",
    "# os.sys(f'gdalwarp {in_file} {out_file} -t_srs \"+proj=longlat +ellps=WGS84\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = gdal.Warp(out_file, in_file, dstSRS='EPSG:4326',\n",
    "#                outputType=gdal.GDT_Int16, xRes=0.00892857142857143, yRes=0.00892857142857143)\n",
    "# ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def project_wsg_shape_to_csr(shape, csr):\n",
    "#     project = lambda x, y: pyproj.transform(\n",
    "#         pyproj.Proj(init='epsg:32618'),\n",
    "#         pyproj.Proj(init=csr),\n",
    "#         x,\n",
    "#         y\n",
    "#       )\n",
    "#     return shapely.ops.transform(project, shape)\n",
    "\n",
    "# yard=project_wsg_shape_to_csr(features, 'epsg:32618')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project = lambda x, y: pyproj.transform(\n",
    "#     pyproj.Proj(init='epsg:3857'),\n",
    "#     pyproj.Proj(init='epsg:4326'),\n",
    "#     x,\n",
    "#     y\n",
    "# )\n",
    "# yard = shapely.ops.transform(project, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project = lambda x, y: pyproj.transform(\n",
    "#     pyproj.Proj(init='epsg:3857'),\n",
    "#     pyproj.Proj(init='epsg:4326'),\n",
    "#     x,\n",
    "#     y\n",
    "# )\n",
    "# yard = shapely.ops.transform(project, geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_vrt = WarpedVRT(image_file, crs='EPSG:3857', resampling=Resampling.bilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_tile = mercantile.tile(*warped_vrt.lnglat(), 9)\n",
    "left, bottom, right, top = mercantile.xy_bounds(*dst_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(left, bottom, right, top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng, lat = image_file.lnglat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lng, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = mercantile.tile(lng, lat, 11)\n",
    "merc_bounds = mercantile.xy_bounds(tile)\n",
    "print(merc_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = gdal.Open('warp_test.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from affine import Affine\n",
    "# import mercantile\n",
    "\n",
    "# import rasterio\n",
    "# from rasterio.enums import Resampling\n",
    "# from rasterio.vrt import WarpedVRT\n",
    "\n",
    "# with rasterio.open('tests/data/RGB.byte.tif') as src:\n",
    "#     with WarpedVRT(src, crs='EPSG:3857',\n",
    "#                    resampling=Resampling.bilinear) as vrt:\n",
    "\n",
    "#         # Determine the destination tile and its mercator bounds using\n",
    "#         # functions from the mercantile module.\n",
    "#         dst_tile = mercantile.tile(*vrt.lnglat(), 9)\n",
    "#         left, bottom, right, top = mercantile.xy_bounds(*dst_tile)\n",
    "\n",
    "#         # Determine the window to use in reading from the dataset.\n",
    "#         dst_window = vrt.window(left, bottom, right, top)\n",
    "\n",
    "#         # Read into a 3 x 512 x 512 array. Our output tile will be\n",
    "#         # 512 wide x 512 tall.\n",
    "#         data = vrt.read(window=dst_window, out_shape=(3, 512, 512))\n",
    "\n",
    "#         # Use the source's profile as a template for our output file.\n",
    "#         profile = vrt.profile\n",
    "#         profile['width'] = 512\n",
    "#         profile['height'] = 512\n",
    "#         profile['driver'] = 'GTiff'\n",
    "\n",
    "#         # We need determine the appropriate affine transformation matrix\n",
    "#         # for the dataset read window and then scale it by the dimensions\n",
    "#         # of the output array.\n",
    "#         dst_transform = vrt.window_transform(dst_window)\n",
    "#         scaling = Affine.scale(dst_window.num_cols / 512,\n",
    "#                                dst_window.num_rows / 512)\n",
    "#         dst_transform *= scaling\n",
    "#         profile['transform'] = dst_transform\n",
    "\n",
    "#         # Write the image tile to disk.\n",
    "#         with rasterio.open('/tmp/test-tile.tif', 'w', **profile) as dst:\n",
    "#             dst.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band1 = image_file.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.plot import show_hist\n",
    "\n",
    "\n",
    "show_hist(reproj_image_file, bins=50, lw=0.0, stacked=False, alpha=0.3, histtype='stepfilled', title=\"Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = feature_df['coordinates'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(coords),type(image_file),type(test_geom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bounds = [rasterio.features.bounds(shape) for shape in test_geom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bounds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx, miny = all_bounds[0][0], all_bounds[0][1]\n",
    "maxx, maxy = all_bounds[0][2], all_bounds[0][3]\n",
    "# minx, miny = 24.60, 60.00\n",
    "# maxx, maxy = 25.22, 60.35\n",
    "bbox = box(minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = gpd.GeoDataFrame({'geometry': bbox}, index=[0], crs=from_epsg(32618))\n",
    "geo = geo.to_crs(crs=image_file.crs.data)\n",
    "coords = get_features(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img, out_transform = mask(image_file, coords, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = image_file.dataset_mask()\n",
    "\n",
    "# Extract feature shapes and values from the array.\n",
    "for geom, val in rasterio.features.shapes(\n",
    "        mask, transform=image_file.transform):\n",
    "\n",
    "    # Transform shapes from the dataset's own coordinate\n",
    "    # reference system to CRS84 (EPSG:4326).\n",
    "    geom = rasterio.warp.transform_geom(\n",
    "        image_file.crs, 'EPSG:4326', geom, precision=6)\n",
    "\n",
    "    # Print GeoJSON shapes to stdout.\n",
    "    print(geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In [17]: array = raster.read()\n",
    "\n",
    "# # Calculate statistics for each band\n",
    "# In [18]: stats = []\n",
    "\n",
    "# In [19]: for band in array:\n",
    "#    ....:     stats.append({\n",
    "#    ....:         'min': band.min(),\n",
    "#    ....:         'mean': band.mean(),\n",
    "#    ....:         'median': np.median(band),\n",
    "#    ....:         'max': band.max()})\n",
    "#    ....: \n",
    "\n",
    "# In [20]: print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
